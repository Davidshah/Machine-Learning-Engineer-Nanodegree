{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Smartcab to Drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "\n",
    "In the not-so-distant future, taxicab companies across the United States no longer employ human drivers to operate their fleet of vehicles. Instead, the taxicabs are operated by self-driving agents — known as smartcabs — to transport people from one location to another within the cities those companies operate. In major metropolitan areas, such as Chicago, New York City, and San Francisco, an increasing number of people have come to rely on smartcabs to get to where they need to go as safely and efficiently as possible. Although smartcabs have become the transport of choice, concerns have arose that a self-driving agent might not be as safe or efficient as human drivers, particularly when considering city traffic lights and other vehicles.\n",
    "\n",
    "To alleviate these concerns, our task is to use reinforcement learning techniques to construct a demonstration of a smartcab operating in real-time to prove that both safety and efficiency can be achieved.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment\n",
    "\n",
    "The smartcab operates in an ideal, grid-like city (similar to New York City), with roads going in the North-South and East-West directions. Other vehicles will certainly be present on the road, but there will be no pedestrians to be concerned with. At each intersection there is a traffic light that either allows traffic in the North-South direction or the East-West direction. U.S. Right-of-Way rules apply:\n",
    "\n",
    "* On a green light, a left turn is permitted if there is no oncoming traffic making a right turn or coming straight through the intersection.\n",
    "* On a red light, a right turn is permitted if no oncoming traffic is approaching from your left through the intersection. To understand how to correctly yield to oncoming traffic when turning left, you may refer to this official drivers’ education video, or this passionate exposition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inputs and Outputs\n",
    "\n",
    "The smartcab has only an egocentric view of the intersection it is at: It can determine the state of the traffic light for its direction of movement, and whether there is a vehicle at the intersection for each of the oncoming directions. For each action, the smartcab may either idle at the intersection, or drive to the next intersection to the left, right, or ahead of it. Finally, each trip has a time to reach the destination which decreases for each action taken (the passengers want to get there quickly). If the allotted time becomes zero before reaching the destination, the trip has failed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rewards and Goal\n",
    "\n",
    "The smartcab receives a reward for each successfully completed trip, and also receives a smaller reward for each action it executes successfully that obeys traffic rules. The smartcab receives a small penalty for any incorrect action, and a larger penalty for any action that violates traffic rules or causes an accident with another vehicle. Based on the rewards and penalties the smartcab receives, the self-driving agent implementation will learn an optimal policy for driving on the city roads while obeying traffic rules, avoiding accidents, and reaching passengers’ destinations in the allotted time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement a Basic Driving Agent\n",
    "\n",
    "Our first task is to get our smartcab moving around our environment. During this task, no attention will be spent on finding the optimal driving policy. Using the `random` module have our smartcab randomly select an action from `[None, 'forward', 'left', 'right']`.\n",
    "\n",
    "`\n",
    "action = random.choice(Environment.valid_actions)\n",
    "`\n",
    "\n",
    "**Question:** What do we see with the agent's behavior as it takes random actions. Does the smartcab eventually make it to the destination? Are there any other interesting observations to note?\n",
    "\n",
    "**Answer**: When taking a random walk through the environment, our smartcab sometimes makes it to the destination and sometimes hits the hard deadline. Either way, it is clear that the route taken is not optimal. Sometimes the smartcab will sit still at an intersection even though there is no oncoming traffic or a red light. Sometimes the smartcab will run a red light with complete disregard for the reward system. When the smartcab does reach the destination, it does not learn anything from the trip. The next trip will again be completely random."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inform the Driving Agent\n",
    "\n",
    "Our next task is to identify a set of states that are appropriate for modeling the smartcab and environment. This is implemented in the following code.\n",
    "\n",
    "    self.next_waypoint = self.planner.next_waypoint()\n",
    "    inputs = self.env.sense(self)\n",
    "    inputs = inputs.items()\n",
    "    self.state = (inputs[0], inputs[1], inputs[3], self.next_waypoint\n",
    "\n",
    "**Question:** What states have we identified that are appropriate for modeling the smartcab and environment? Why do we believe each of these states to be appropriate for this problem?\n",
    "\n",
    "**Answer**: I chose to use a combination of environmental inputs as an implicit state for our smartcab. The first input I chose was `'light'`. If  `'light'` is red, I know I can turn right or stay still. If `'light'` is green, I know I can turn right, go straight, or maybe turn left. The second input I chose was `'oncoming'`. If there is oncoming traffic and the light is green, I know I can’t turn left. The third input I chose was `'left'`. If there is traffic coming from the left and the light is red, I know I can’t turn right. The fourth input I used was `'next_waypoint'`. This is the location of the target destination relative to our current position and will be useful for deciding how to act optimally given a time restraint.\n",
    "\n",
    "I did not include the input `'right'` since traffic coming from the right never affects our decision. I considered using `'deadline'` but felt that this should be irrelevant. It should be captured in the reward system and I do not want my agent to start breaking laws when time is running out. Also, it would blow up our state space causing us to suffer from the curse of dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement a Q-Learning Driving Agent\n",
    "\n",
    "Now that our driving agent can interpret input information, our next task is to implement a Q-Learning algorithm for our driving agent to choose the best action at each step. This will be based on the reward given for each action at each step. To implement this, I have created a `Qtable` `class` that can be used to map Q-values to a table.\n",
    "\n",
    "\n",
    "    class QTable(object):\n",
    "        \"\"\" \n",
    "        Table to store Q-learning data Q(s,a).  \n",
    "        Does not scale well with increasing sizes of state/action space.\n",
    "        \"\"\"\n",
    "    \n",
    "        def __init__(self):\n",
    "            self.Qhat = dict()\n",
    "\n",
    "        def get(self, state, action):\n",
    "            key = (state, action)\n",
    "            return self.Qhat.get(key, None)\n",
    "\n",
    "        def set(self, state, action, q):\n",
    "            key = (state, action)\n",
    "            self.Qhat[key] = q\n",
    "\n",
    "**Question:** What changes do we notice in the agent's behavior when compared to the basic driving agent when random actions were always taken? Why is this behavior occurring?\n",
    "\n",
    "**Answer:** It quickly becomes apparent that the Q-Learning is giving our smartcab a better appreciation of the environment. For example, the smartcab is actually making an effort to reach the destination. Before, the smartcab would often sit still at a green light but that happens less frequently now. The smartcab is also beginning to understand traffic rules, avoiding the punishments of running red lights.\n",
    "\n",
    "This behavior is occuring because as the q-table is updated throughout trials, the agent starts to understand what it gets rewarded for and what it gets punished for. Through this reinforcement, the agent starts to formulate a policy for maximizing value at each state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improve the Q-Learning Driving Agent\n",
    "\n",
    "Our final task is to enchance our driving agent so that, after sufficient training, the smartcab is able to reach the destination within the allotted time safely and efficiently. To implement this we will include `alpha`, `gamma`, and `epsilon` as parameters for our Q-Learning algorithm. The main chunk of learning code is:\n",
    "\n",
    "    # Q(s,a) <-- Q(s,a) + alpha * (r + gamma * maxQ(s', a') - Q(s,a))\n",
    "    # Calculate maxQ(s', a')\n",
    "    next_q = [self.Qhat.get(next_state, a) for a in self.val_actions]\n",
    "    future_util = max(next_q)         \n",
    "    if future_util is None:\n",
    "        future_util = 0.0\n",
    "\n",
    "    # Get current q from Qtable\n",
    "    q = self.Qhat.get(self.state, action)\n",
    "        \n",
    "    # Update q through value iteration, setting initial q to reward\n",
    "    if q is None:\n",
    "        q = reward\n",
    "    else:\n",
    "        # Old value + learning rate * (reward + discount * est future value - old value)\n",
    "        q += self.alpha * (reward + self.gamma * future_util - q)\n",
    "\n",
    "    self.Qhat.set(self.state, action, q)\n",
    "        \n",
    "**Question:** Report the different values for the parameters tuned in our basic implementation of Q-Learning. For which set of parameters does the agent perform best? How well does the final driving agent perform?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "| `alpha` | `gamma` | `epsilon` | trial 1 #passed | trial 2 #passed | trial 3 #passed | average |\n",
    "|---------|---------|-----------|-----------------|-----------------|-----------------|---------|\n",
    "| 1 --> 0 | 0.5     | 0.9       | 24              | 27              | 24              | 25      |\n",
    "| 1 --> 0 | 0.5     | 0.5       | 61              | 51              | 68              | 60      |\n",
    "| 1 --> 0 | 0.5     | 0.05      | 81              | 63              | 76              | 73      |\n",
    "| 1 --> 0 | 0.5     | 0.1       | 71              | 85              | 80              | 79      |\n",
    "| 1 --> 0 | 0.7     | 0.1       | 79              | 76              | 89              | 81      |\n",
    "| 1 --> 0 | 0.17    | 0.05      | 97              | 79              | 76              | 84      |\n",
    "| 1 --> 0 | 0.1     | 0.1       | 93              | 83              | 90              | 89      |\n",
    "| 1 --> 0 | 0.3     | 0.1       | 93              | 87              | 89              | 90      |\n",
    "| 1 --> 0 | 0.2     | 0.05      | 82              | 98              | 92              | 91      |\n",
    "| 1 --> 0 | 0.23    | 0.05      | 83              | 98              | 91              | 91      |\n",
    "| 1 --> 0 | 0.2     | 0.1       | 94              | 93              | 86              | 91      |\n",
    "| 1 --> 0 | 0.25    | 0.05      | 87              | 93              | 96              | 92      |\n",
    "\n",
    "It is worth going over how `alpha`, `gamma`, and `epsilon` affect our algorthim.\n",
    "* `alpha`: The learning rate determines the extent that new information will override old information. A factor of 0 means the agent will not learn anything, while a factor of 1 means the agent will only consider the newest information. I felt that learning should diminish over time so I set `alpha` to start at 1 and then approach zero over the number of trials.\n",
    "    * `self.alpha = 1 / self.counter`\n",
    "* `gamma`: The discount factor determines how important future rewards will be. A factor of 0 means the agent will only consider immediate rewards, while a factor approaching 1 means the agent will strive for a long-term high reward. Through trial and error I arrived at a value of 0.25 for `gamma`\n",
    "    * `self.gamma = 0.25`\n",
    "* `epsilon`: The exploration factor determines how frequent we choose a random action. A factor of 0 means the agent will always use our Q-value and won't learn, while a factor of 1 means the agent will always choose randomly and won't use what we've learned. Through trial and error I arrived at a value of 0.05 for `epsilon`.\n",
    "    * `self.epsilon = 0.05`\n",
    "    \n",
    "Our final agent performs fairly well. Of the 100 trials, the smartcab reached it's destination on time 92% of the time.\n",
    "\n",
    "**Question:** Does our agent get close to finding an optimal policy, i.e. reach the destination in the minimum possible time, and not incur any penalties? How would you describe an optimal policy for this problem?\n",
    "\n",
    "**Answer:** Our agent is successful 92% of the time and on average the last 10 trials are passed. However, there are still some issues with the agent. Consider the following table showing times the agent recieves a negative reward in the last 10 trials.\n",
    "\n",
    "| trial |                                      input                                     | waypoint |  action | reward | destination reached |\n",
    "|:-----:|:------------------------------------------------------------------------------:|:--------:|:-------:|:------:|:-------------------:|\n",
    "|   91  | [('light', 'green'), ('oncoming', None), ('right', None), ('left', 'forward')] |   right  | forward |  -0.5  |         yes         |\n",
    "|   92  |     [('light', 'red'), ('oncoming', None), ('right', None), ('left', None)]    |  forward |   left  |  -1.0  |         yes         |\n",
    "|   93  |     [('light', 'red'), ('oncoming', None), ('right', None), ('left', None)]    |  forward |  right  |  -0.5  |         yes         |\n",
    "|   94  |     [('light', 'red'), ('oncoming', None), ('right', None), ('left', None)]    |  forward |  right  |  -0.5  |         yes         |\n",
    "|   96  |  [('light', 'red'), ('oncoming', None), ('right', None), ('left', 'forward')]  |  forward |   left  |  -1.0  |         yes         |\n",
    "\n",
    "This would not pass muster in a real world setting. Four red lights are ran through and the optimal path isn't always taken. Despite these issues, I still think it is possible to find an optimal policy with some more experimentation. I could try decaying `epsilon`, creating new tiebreakers, and changing the initial states of the q-values.\n",
    "\n",
    "I would describe an optimal policy as:\n",
    "1. Choose the path that minimizes travel distance to the waypoint.\n",
    "2. Follow that path while observing traffic laws and avoiding collisions.\n",
    "3. Repeat"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:python2]",
   "language": "python",
   "name": "conda-env-python2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
